{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Flower - Federated Learning\n",
    "\n",
    "O tutorial tem o objetivo de comparar o aprendizado de máquina tradicional com o federado. Ele está disponível em: https://colab.research.google.com/github/adap/flower/blob/main/examples/flower-in-30-minutes/tutorial.ipynb#scrollTo=gLucT6KFnIT7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Configurando o ambiente de aprendizado*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /home/manuelamfo/my_venv/lib/python3.12/site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/manuelamfo/my_venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/manuelamfo/my_venv/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (70.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# instalo flower (framework para aprendizado federado), matplotlib e pytorch\n",
    "%pip install -q \"flwr[simulation]\" flwr-datasets\n",
    "%pip install matplotlib\n",
    "%pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento centralizado: a maneira antiga de se fazer ML\n",
    "\n",
    "Para ilustrar as diferenças entre a maneira tradicional de se fazer aprendizado de máquina e a forma federada, primeiro vamos criar um modelo típico de CNN, com um loop de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importação das bibliotecas necessárias para o dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_mnist_dataloaders(mnist_dataset, batch_size : int):\n",
    "    pytorch_transforms = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    # preparo as funções de transformação\n",
    "    def apply_transforms(batch):\n",
    "        batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
    "        return batch\n",
    "    \n",
    "    mnist_train = mnist_dataset[\"train\"].with_transform(apply_transforms)\n",
    "    mnist_test = mnist_dataset[\"test\"].with_transform(apply_transforms)\n",
    "\n",
    "    # construo os dataloaders do pytorch\n",
    "    trainloader = DataLoader(mnist_train, batch_size=batch_size, shuffle = True)\n",
    "    testloader = DataLoader(mnist_test, batch_size=batch_size)\n",
    "    return trainloader, testloader\n",
    "\n",
    "# faz o download do conjuento de dados\n",
    "mnist = load_dataset(\"ylecun/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 60000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visão rápida do conjunto de dados\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizo a distribuição dos dados de treino por meio de um histograma\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# construo o histograma\n",
    "all_labels = mnist[\"train\"][\"label\"]\n",
    "all_labels_counts = Counter(all_labels)\n",
    "\n",
    "# visualizo o histograma\n",
    "bar = plt.bar(all_labels_counts.keys(), all_labels_counts.values())\n",
    "_ = plt.bar_label(bar)\n",
    "\n",
    "# formação da plotagem\n",
    "_ = plt.xticks([label for label in all_labels_counts.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will display images with idx: [36959, 35239, 43986, 57364, 15436, 27516, 45884, 27050, 41169, 16233, 36730, 59044, 34271, 16183, 18683, 38880, 30459, 34760, 25881, 50223, 15464, 26632, 44386, 31317, 30109, 4142, 13278, 48267, 12568, 51816, 42313, 27220]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizo 32 imagens do conjunto de dados\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def visualise_n_random_examples(trainset_, n: int, verbose: bool = True):\n",
    "  trainset_data = [Image.open(io.BytesIO(entry[0].as_py())) for entry in trainset_.data[0]] # consigo as imagens\n",
    "  idx = list(range(len(trainset_data))) # crio uma lista de índices em order crescente com base no tamanho do meu conjunto de dados de treino\n",
    "  random.shuffle(idx) # embaralho os índices da lista\n",
    "  idx = idx[:n] # pego os primeiros n índices da lista embaralhada\n",
    "  if verbose:\n",
    "    print(f\"will display images with idx: {idx}\")\n",
    "\n",
    "  # construo o quadro que será utilizado para imprimir as imagens\n",
    "  num_cols = 8\n",
    "  num_rows = int(np.ceil(len(idx) / num_cols))\n",
    "  fig, axs = plt.subplots(figsize=(16, num_rows * 2), nrows=num_rows, ncols=num_cols)\n",
    "\n",
    "  # exibo as imagens no quadro\n",
    "  for c_i, i in enumerate(idx):\n",
    "    axs.flat[c_i].imshow(trainset_data[i], cmap=\"gray\")\n",
    "\n",
    "visualise_n_random_examples(mnist[\"train\"], n=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uma arquitetura CNN*\n",
    "\n",
    "Uma arquitetura CNN típica é usada para ilustrar a diferença entre a maneira tradicinal e a federada de se fazer aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# modelo da rede neural convolucional a ser usado como base para a comparação\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, num_classes: int) -> None:\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear( 16 * 4 * 4, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16 * 4 * 4)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters = 44426\n"
     ]
    }
   ],
   "source": [
    "model = Net(num_classes=10) # inicializa o modelo com 10 classes de saída\n",
    "num_parameters = sum(value.numel() for value in model.state_dict().values()) # soma o número de elementos em cada tensor dos parâmetros (pesos e vieses) do modelo\n",
    "print(f\"{num_parameters = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*O loop de treino*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino as funções de treino, teste e a que irá rodar o modelo\n",
    "\n",
    "def train (net, trainloader, optimizer, device=\"cpu\"):\n",
    "  \"\"\"Treino o modelo nos dados de treino\"\"\"\n",
    "  criterion = torch.nn.CrossEntropyLoss() # loss function -> calcula a diferença entre a predição do modelo com o resultado dos dados reais\n",
    "  net.to(device) # envio o modelo para o dispositivo desejado\n",
    "  net.train() # coloco o modelo em modo de treino\n",
    "  for batch in trainloader:\n",
    "    images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(net(images), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(net, testloader, device):\n",
    "  \"\"\"Validação da rede neutral com base no conjunto de dados de teste\"\"\"\n",
    "  criterion = torch.nn.CrossEntropyLoss()\n",
    "  correct, loss = 0, 0\n",
    "  net.to(device)\n",
    "  net.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "      images, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "      outputs = net(images)\n",
    "      loss += criterion(outputs, labels).item()\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "  accuracy = correct / len(testloader.dataset)\n",
    "  return loss, accuracy\n",
    "\n",
    "def run_centralised(trainloader, testloader, epochs: int, lr: float, momentum: float = 0.9):\n",
    "  # Um loop de treino centralizado completo\n",
    "\n",
    "  # instanciação do modelo\n",
    "  model = Net(num_classes=10)\n",
    "\n",
    "  # descoberta de qual dispositivo está disponível e envio do modelo para lá\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model.to(device)\n",
    "\n",
    "  # definição do otimizador juntamente com os hiperparâmetros\n",
    "  optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "  # loop de treino\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"Training epoch {epoch} ...\")\n",
    "    train(model, trainloader, optim, device)\n",
    "\n",
    "  # o treino está completo, então avalia-se o modelo no conjunto de teste\n",
    "  loss, accuracy = test(model, testloader, device)\n",
    "  print(f\"{loss =}\")\n",
    "  print(f\"{accuracy =}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ...\n",
      "Training epoch 1 ...\n",
      "Training epoch 2 ...\n",
      "loss =10.779221353732282\n",
      "accuracy =0.9891\n"
     ]
    }
   ],
   "source": [
    "# construção dos dataloaders\n",
    "trainloader, testloader = get_mnist_dataloaders(mnist, batch_size=32)\n",
    "\n",
    "# rodo o treinamento centralizado\n",
    "run_centralised(trainloader, testloader, epochs=3, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No treinamento centralizado, a CNN alcançou 0.987% de acurácia, o que já era esperado. Apesar de tal forma de treinamento ser muito útil em várias aplicações, em algumas ela deixa a desejar em ambientes em que não se é possível coletar os dados para se criar um ambiente centralizado de aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O futuro da IA é federado\n",
    "\n",
    "No aprendizado federado, os dados de cada cliente permanecem privados e o treinamento ocorre de forma distribuída.\n",
    "\n",
    "Agora, iremos modificar o loop de treino tradicional que fizemos para construir uma pipeline federada de aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Um cliente, uma partição dos dados*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para rodar o treinamento federado, é necessário importar o flower e dividir os dados entre os clientes de forma aleatória, para simular um ambiente real\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "NUM_PARTITIONS = 100 # quantidade de clientes\n",
    "\n",
    "partitioner = IidPartitioner(num_partitions=NUM_PARTITIONS) # cria um particionador que divide os dados em 100 partes aleatórias e IID\n",
    "# com base nessa partição aleatória em 100 partes, o conjunto de dados MNIST, na seção de treino, é dividido entre si\n",
    "fds = FederatedDataset(dataset=\"ylecun/mnist\", partitioners={\"train\": partitioner})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 600\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para acessar os dados de um cliente específico, basta acessar o índice desejado\n",
    "partition_0 = fds.load_partition(0)\n",
    "partition_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# para visualizar como os dados estão particionados entre os clientes, é interessante plotar um gráfico comparando a distribuição dos dados de treino entre os clientes\n",
    "from flwr_datasets.visualization import plot_label_distributions\n",
    "\n",
    "fig, ax, df = plot_label_distributions(\n",
    "    partitioner,\n",
    "    label_name=\"label\",\n",
    "    plot_type=\"bar\",\n",
    "    size_unit=\"absolute\",\n",
    "    partition_id_axis=\"x\",\n",
    "    legend=True,\n",
    "    verbose_labels=True,\n",
    "    max_num_partitions=30, # apenas os 30 primeiros clientes são exibidos\n",
    "    title=\"Per partition label distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definindo um cliente no aprendizado federado*\n",
    "\n",
    "Um cliente em FL é uma entidade que possui alguns dados e treina o modelo utilizando-os. Um cliente flower é uma classe em Python que possui dois métodos distintos:\n",
    "> fit(): com esse método, o cliente treina o modelo localmente por um determinado número de épocas, utilizando os próprios dados. No final, o modelo resultante é enviado de volta ao servidor para agregação\n",
    "\n",
    "> evaluate(): com esse método, o server consegue avaliar a performance do modelo global com base no conjunto de validação local de um cliente. Isso pode ser usado, por exemplo, quando não há um dataset centralizado no servidor para validação/teste. Além disso, esse método pode ser usado para avaliar o grau de personalização do modelo federado (isto é, o quão parecido o modelo global está daquele modelo loval)\n",
    "\n",
    "O nome da classe de clientes flower aqui é FlowerClient. Cada cliente no sistema de aprendizado federado é representado por uma instância de FlowerClient*. Essa classe será futuramente envolta em um uma classe chamada ClientApp (própria do Flower), que lida com a lógica do treinamento por parte do cliente* (*referência: <https://www.kaggle.com/code/nechbamohammed/the-flower-federated-learning-tutorial-part-1>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from flwr.common import NDArrays, Scalar\n",
    "from flwr.client import NumPyClient\n",
    "import torch.optim.sgd\n",
    "\n",
    "# cada cliente flower possui um modelo, os dados de treino e teste particionados em batches, e o dispositivo definido\n",
    "# além disso, possui as funções de treino e teste, que são as mesmas do treinamento centralizado\n",
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, trainloader, valloader) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model = Net(num_classes=10) # para treinar o modelo nos clientes, é necessário instanciar a CNN normalmente\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # descobrir qual dispositivo está disponível\n",
    "\n",
    "    def fit(self, parameters, config): # mesma função de treino do treinamento centralizado\n",
    "        \"\"\" Esse método treina o modelo nos dados locais, com base nos parâmetros enviados pelo servidor.\n",
    "        No final, os parÂmetros do modelo local treinado são enviados de volta ao servidor\"\"\"\n",
    "\n",
    "        # copia os parâmetros do servidor para o modelo local\n",
    "        set_params(self.model, parameters)\n",
    "\n",
    "        # define o optimizer\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # treina o modelo local (uso a mesma função do treino centralizado)\n",
    "        train(self.model, self.trainloader, optim, self.device)\n",
    "\n",
    "        # retorna os parâmetros do modelo treinado, junto com a quanrtidade de exemplos usados no treino\n",
    "        return get_params(self.model), len(self.trainloader), {}\n",
    "    \n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]): # mesma função de teste do treinamento centralizado\n",
    "        \"\"\"Esse método avalia o modelo nos dados locais de validação, com base nos parâmetros enviados pelo servidor\"\"\"\n",
    "\n",
    "        set_params(self.model, parameters) # mudança dos parâmetros do modelo local para os parâmetros do servidor\n",
    "\n",
    "        # avalia o modelo local (uso a mesma função de teste)\n",
    "        loss, accuracy = test(self.model, self.valloader, self.device)\n",
    "\n",
    "        # retorna a loss e a acurácia do modelo local para o servidor\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}\n",
    "    \n",
    "# Duas funções auxiliares para configurar e extrair os parâmetros do modelo\n",
    "def set_params(model, parameters):\n",
    "    \"\"\" Troca os parâmetros do modelo local pelo conjunto de parâmetros enviado pelo servidor\"\"\"\n",
    "\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    model.load_state_dict(state_dict) # troco os parâmetros\n",
    "\n",
    "def get_params(model):\n",
    "    \"\"\" Extraio os parâmetros do modelo local como uma lista de NumPy arrays\"\"\"    \n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As instâncias de FlowerClient são criadas pelo Flower à medida em que são necessárias na parte de treinamento e testagem. (https://www.kaggle.com/code/nechbamohammed/the-flower-federated-learning-tutorial-part-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*O callback de client_fn*\n",
    "\n",
    "Para simular uma organização com a sua própria partição dos dados, usa-se a função chamada client_fn. Essa função cria instâncias de FlowerClient de acordo com a necessidade e usa a variável partition_id para carregar partições específicas dos dados em cada cliente (https://www.kaggle.com/code/nechbamohammed/the-flower-federated-learning-tutorial-part-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import Context\n",
    "from flwr.client import ClientApp\n",
    "\n",
    "def client_fn(context: Context):\n",
    "    \"\"\" Retorna uma instância de FlowerClient, representando um único cliente, contendo a sua partição dos dados \"\"\"\n",
    "\n",
    "    partition_id = int(context.node_config[\"partition-id\"])\n",
    "    partition = fds.load_partition(partition_id, \"train\")\n",
    "    # partição entre dados de treino e validação\n",
    "    partition_train_val = partition.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    # construção dos dataloaders a partir da partição feita entre treino e validação para um único cliente\n",
    "    trainloader, testloader = get_mnist_dataloaders(partition_train_val, batch_size=32)\n",
    "\n",
    "    return FlowerClient(trainloader=trainloader, testloader= testloader).to_client() # passagem dos parâmetros para o método __init__ de FlowerClient, contendo os dados de treino e teste particionados em batches referentes a um único cliente\n",
    "\n",
    "# construção do ClientApp, com base na função client_fn. Essa função é chamada para cada cliente que se conecta ao servidor\n",
    "client_app = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo um ServerApp no Flower\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definição da estratégia de agregação*\n",
    "Se na parte do ClientApp definimos como ocorrerá a partição dos dados, na parte do ServerApp estabelecemos como se dará a estratégia de agregação deles. Aqui, utilizaremos a estratégia de Federated Averaging (FedAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from flwr.common import Metrics\n",
    "\n",
    "# definição da métrica da função de agregação\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # multiplicação da acurácia de cada cliente pelo número de exemplos usados\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # agregação e retorno das métrica customizada (média ponderada)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*O callback de server_fn*\n",
    "\n",
    "A classe estabelece o que acontecerá por parte do servidor quando um cliente se conectar a ele. Aqui, ela utiliza a função server_fn como base para as suas operações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import ndarrays_to_parameters\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "\n",
    "num_rounds = 5\n",
    "\n",
    "def server_fn(context: Context):\n",
    "\n",
    "    # instancio o modelo feito no treinamento centralizado, já que essa parte de construção de modelos não muda no treinamento federado\n",
    "    model = Net(num_classes=10)\n",
    "    ndarrays = get_params(model)\n",
    "    # converto os parâmetros do modelo (em NumPy) para o formato do Flower\n",
    "    global_model_init = ndarrays_to_parameters(ndarrays)\n",
    "\n",
    "    # definição da estatrégia \n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=0.1, # tamanho da amostra de clientes que serão usados para rodar o método fit() - treino\n",
    "        fraction_evaluate=0.5, # tamanho da amostra de clientes que serão usados para rodar o método evaluate() - validação\n",
    "        evaluate_metrics_aggregation_fn=weighted_average, # função de agregação das métricas definida anteriormente\n",
    "        initial_parameters=global_model_init # modelo global inicial\n",
    "    )\n",
    "\n",
    "    # construção do ServerConfig\n",
    "    config = ServerConfig(num_rounds=num_rounds)\n",
    "\n",
    "    # coloco tudo dentro de um objeto ServerAppComponents -> não preciso passar o modelo como parâmetro, pois ele não será usado, apenas os parâmetros que ele gera são usados no cálculo do FedAvg\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# construção do ServerApp, com base na função server_fn. Essa função é chamada para cada cliente que se conecta ao servidor\n",
    "server_app = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lançamento da simulação\n",
    "\n",
    "Como as classes ClientApp e ServerApp foram definidas, nós podemos lançar a simulação de um treinamento em ambiente federado. Para tal, basta passar essas classes para a função run_simulation(), juntamente com a quantidade de supernodes/clientes do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=12430) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flwr[simulation] in /home/manuelamfo/my_venv/lib/python3.12/site-packages (1.16.0)\n",
      "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (44.0.2)\n",
      "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (1.67.1)\n",
      "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (0.0.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (1.26.4)\n",
      "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (0.12.1)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (4.25.5)\n",
      "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (3.21.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (6.0.2)\n",
      "Requirement already satisfied: ray==2.31.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (2.31.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (2.32.3)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (13.9.4)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (2.0.2)\n",
      "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (1.1.0)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from flwr[simulation]) (0.12.5)\n",
      "Requirement already satisfied: click>=7.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (8.1.7)\n",
      "Requirement already satisfied: filelock in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (3.16.1)\n",
      "Requirement already satisfied: jsonschema in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (4.22.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
      "Requirement already satisfied: packaging in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (24.1)\n",
      "Requirement already satisfied: aiosignal in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from ray==2.31.0->flwr[simulation]) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.12.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/manuelamfo/my_venv/lib/python3.12/site-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U \"flwr[simulation]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[36m(ClientAppActor pid=12999)\u001b[0m /home/manuelamfo/my_venv/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "\u001b[36m(ClientAppActor pid=12999)\u001b[0m   obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 10 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 50 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 50 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 10 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 50 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 50 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 10 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 50 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 50 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 10 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 50 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 50 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 10 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 10 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 50 clients (out of 100)\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 143, in __call__\n",
      "    return self._call(message, context)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/client_app.py\", line 124, in ffn\n",
      "    out_message = handle_legacy_message_from_msgtype(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/client/message_handler/message_handler.py\", line 96, in handle_legacy_message_from_msgtype\n",
      "    client = client_fn(context)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12430/2912974451.py\", line 15, in client_fn\n",
      "TypeError: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=12999, ip=172.17.171.38, actor_id=4d477dde328d3c6528b7d51c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7fb36888b5c0>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
      "    raise ClientAppException(str(ex)) from ex\n",
      "flwr.client.client_app.ClientAppException: \n",
      "Exception ClientAppException occurred. Message: FlowerClient.__init__() got an unexpected keyword argument 'testloader'\n",
      "\n",
      "\u001b[91mERROR \u001b[0m:     An exception was raised when processing a message by RayBackend\n",
      "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/vce_api.py\", line 111, in worker\n",
      "    out_mssg, updated_context = backend.process_message(message, context)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 187, in process_message\n",
      "    raise ex\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py\", line 175, in process_message\n",
      "    ) = self.pool.fetch_result_and_return_actor_to_pool(future)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py\", line 478, in fetch_result_and_return_actor_to_pool\n",
      "    _, out_mssg, updated_context = ray.get(future)\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2639, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 866, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.17.171.38, ID: 64d87046967990bc37bdccb06ec149c58fc443b88fad61eda031e313) where the task (actor ID: 9c05f60a472015bd5a73395c01000000, name=ClientAppActor.__init__, pid=13000, memory used=0.42GB) was running was 3.59GB / 3.77GB (0.95051), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.17.171.38`. To see the logs of the worker, use `ray logs worker-0b16e0f3c04a669936f2ea090868fc3cc7555607b40286b50c3c9d68*out -ip 172.17.171.38. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "388\t0.66\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "169\t0.60\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node --dns-result-order...\n",
      "12430\t0.47\t/home/manuelamfo/my_venv/bin/python -m ipykernel_launcher --f=/mnt/wslg/runtime-dir/jupyter/runtime/...\n",
      "12999\t0.42\tray::ClientAppActor.run\n",
      "13000\t0.42\tray::ClientAppActor.run\n",
      "12716\t0.06\t/home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp...\n",
      "12748\t0.05\t/home/manuelamfo/my_venv/bin/python /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/dashbo...\n",
      "73\t0.05\t/home/manuelamfo/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/node /home/manuelamfo/....\n",
      "12822\t0.05\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/das...\n",
      "12747\t0.04\t/home/manuelamfo/my_venv/bin/python -u /home/manuelamfo/my_venv/lib/python3.12/site-packages/ray/aut...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 50 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 31.57s\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "from flwr.simulation import run_simulation\n",
    "\n",
    "run_simulation(server_app=server_app, client_app=client_app, num_supernodes=NUM_PARTITIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
